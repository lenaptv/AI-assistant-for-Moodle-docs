{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5sDiOsi-B9Gk"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jNq9uqsRB-c"
      },
      "outputs": [],
      "source": [
        "pip install -U langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5kejEnbRDV-"
      },
      "outputs": [],
      "source": [
        "pip install chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FC9DAJ-5J37a"
      },
      "outputs": [],
      "source": [
        "pip install pyngrok"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvUnlQ2a5ijc"
      },
      "source": [
        "# RAG-pipeline: Qwen2.5-0.5B+hybrid_search+reranker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sf9ZyusW5egD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from fastapi.responses import JSONResponse\n",
        "from pydantic import BaseModel, Extra\n",
        "from typing import Optional, List, Any\n",
        "\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.llms.base import LLM\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.schema import BaseRetriever, Document\n",
        "from langchain.retrievers import EnsembleRetriever\n",
        "\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, AutoModel\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# === LLM ===\n",
        "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\", device_map=\"auto\")\n",
        "llm_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=512, temperature=0.01)\n",
        "\n",
        "SYSTEM_PROMPT = (\n",
        "    \"Ты помощник по документации Moodle. \"\n",
        "    \"Отвечай кратко, точно и по теме. \"\n",
        "    \"Если ответа нет в контексте, используй общие знания и постарайся помочь. \"\n",
        "    \"Отвечай на том языке, на котором задан вопрос.\\n\\n\"\n",
        ")\n",
        "\n",
        "class LocalLLM(LLM):\n",
        "    class Config:\n",
        "        extra = Extra.allow\n",
        "\n",
        "    def __init__(self, pipeline: Any):\n",
        "        super().__init__()\n",
        "        self.__dict__[\"pipeline\"] = pipeline\n",
        "\n",
        "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
        "        try:\n",
        "            full_prompt = SYSTEM_PROMPT + prompt\n",
        "            output = self.pipeline(full_prompt)[0][\"generated_text\"]\n",
        "            return output[len(prompt):]\n",
        "        except Exception:\n",
        "            return \"Произошла ошибка при генерации ответа.\"\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"local_llm\"\n",
        "\n",
        "# === Embeddings ===\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"Qwen/Qwen3-Embedding-0.6B\")\n",
        "\n",
        "# === ChromaDB ===\n",
        "chroma_path = \"/content/drive/MyDrive/RAG_Moodle/chroma_db_qwen3\"\n",
        "db = Chroma(persist_directory=chroma_path, embedding_function=embedding_model)\n",
        "\n",
        "# === Memory ===\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "# === BGE reranker ===\n",
        "reranker_name = \"BAAI/bge-reranker-v2-m3\"\n",
        "reranker_tokenizer = AutoTokenizer.from_pretrained(reranker_name)\n",
        "reranker_model = AutoModel.from_pretrained(reranker_name)\n",
        "\n",
        "def rerank(query: str, documents: List[str], top_n: int = 8) -> List[str]:\n",
        "    try:\n",
        "        if not documents:\n",
        "            return []\n",
        "\n",
        "        pairs = [(query, doc) for doc in documents]\n",
        "        inputs = reranker_tokenizer(pairs, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = reranker_model(**inputs)\n",
        "            if hasattr(outputs, \"logits\"):\n",
        "                scores = outputs.logits.view(-1)\n",
        "            else:\n",
        "                scores = outputs.last_hidden_state[:, 0, :].mean(dim=1)\n",
        "\n",
        "        top_indices = torch.topk(scores, k=min(top_n, len(documents))).indices.tolist()\n",
        "        return [documents[i] for i in top_indices]\n",
        "    except Exception:\n",
        "        return documents[:top_n]\n",
        "\n",
        "# === Кастомный RAG Retriever с rerank ===\n",
        "class RerankRetriever(BaseRetriever):\n",
        "    def __init__(self, base_retriever, reranker_model, reranker_tokenizer, top_k=8):\n",
        "        super().__init__()\n",
        "        object.__setattr__(self, \"base_retriever\", base_retriever)\n",
        "        object.__setattr__(self, \"reranker_model\", reranker_model)\n",
        "        object.__setattr__(self, \"reranker_tokenizer\", reranker_tokenizer)\n",
        "        object.__setattr__(self, \"top_k\", top_k)\n",
        "\n",
        "    def get_relevant_documents(self, query: str) -> List[Document]:\n",
        "        try:\n",
        "            docs = self.base_retriever.get_relevant_documents(query)\n",
        "            texts = [doc.page_content for doc in docs]\n",
        "            top_texts = rerank(query, texts, top_n=self.top_k)\n",
        "            return [doc for doc in docs if doc.page_content in top_texts]\n",
        "        except Exception:\n",
        "            return []\n",
        "\n",
        "\n",
        "# === Гибридный Retriever ===\n",
        "vector_retriever = db.as_retriever(search_kwargs={\"k\": 8})\n",
        "keyword_retriever = db.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 8})\n",
        "\n",
        "hybrid_retriever = EnsembleRetriever(\n",
        "    retrievers=[vector_retriever, keyword_retriever],\n",
        "    weights=[0.5, 0.5]\n",
        ")\n",
        "\n",
        "# === RAG с rerank ===\n",
        "retriever_with_rerank = RerankRetriever(\n",
        "    base_retriever=hybrid_retriever,\n",
        "    reranker_model=reranker_model,\n",
        "    reranker_tokenizer=reranker_tokenizer,\n",
        "    top_k=8\n",
        ")\n",
        "\n",
        "local_llm = LocalLLM(pipeline=llm_pipeline)\n",
        "\n",
        "qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "    llm=local_llm,\n",
        "    retriever=retriever_with_rerank,\n",
        "    memory=memory\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6V7Nzj5RQAW"
      },
      "source": [
        "# FastAPI - приложение"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUTTw3zKJjp4"
      },
      "outputs": [],
      "source": [
        "from fastapi import FastAPI\n",
        "from pyngrok import ngrok, conf\n",
        "import nest_asyncio\n",
        "import uvicorn\n",
        "import os\n",
        "\n",
        "# Вставь сюда свой токен\n",
        "conf.get_default().auth_token = \"ВАШ_ТОКЕН\"\n",
        "\n",
        "# === FastAPI ===\n",
        "app = FastAPI()\n",
        "\n",
        "class Question(BaseModel):\n",
        "    query: str\n",
        "\n",
        "@app.post(\"/ask\")\n",
        "def ask_q(input: Question):\n",
        "    try:\n",
        "        if not input.query or input.query.strip() == \"\":\n",
        "            raise HTTPException(status_code=400, detail=\"Запрос не может быть пустым.\")\n",
        "\n",
        "        result = qa_chain.run(input.query)\n",
        "        return {\"answer\": result}\n",
        "\n",
        "    except HTTPException as http_err:\n",
        "        raise http_err\n",
        "\n",
        "    except Exception:\n",
        "        return JSONResponse(\n",
        "            status_code=500,\n",
        "            content={\"error\": \"Произошла ошибка при генерации ответа.\"}\n",
        "        )\n",
        "\n",
        "\n",
        "# Подключение ngrok\n",
        "public_url = ngrok.connect(8000)\n",
        "print(f\"Public URL: {public_url}\")\n",
        "\n",
        "# Запуск сервера\n",
        "nest_asyncio.apply()\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6AeS3pjRzCV"
      },
      "source": [
        "# Локальный вывод"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMlJAvAtHX3q"
      },
      "outputs": [],
      "source": [
        "result1 = qa_chain.run(\"Как создать новый курс в Moodle?\")\n",
        "print(result1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7DR7eRCIBU7"
      },
      "outputs": [],
      "source": [
        "result2 = qa_chain.run(\"Как настроить систему оценок в Moodle?\")\n",
        "print(result2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvM3EAXaIEGh"
      },
      "outputs": [],
      "source": [
        "result3 = qa_chain.run(\"Как просмотреть журналы активности пользователей?\")\n",
        "print(result3)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
